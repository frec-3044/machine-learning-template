---
title: "Machine learning steps"
author: "Quinn Thomas"
date: "2023-03-11"
output: github_document
---

## Step 1: Obtain data

Read in data to memory.  In the course we have accessed csv, excel, and database forms located in the github repo, on websites, and through an API.

## Step 2: Pre-process data

Split data: divide data into the training and test sets. [WHY]

Recipes: Modify predictors/features/covariates/independent variables for analysis

  - Modify so that correctly formatted for a particular model. For example, a linear regression requires groups to be dummy variables. Therefore, a column called "ecosystem" with two values: "forest" and "grass" would be converted to two columns: ecosystem_forest with a value of 0 or 1 and ecosystem_grass with a value of 0 and 1)
  - Removing highly correlated predictors
  - Rescaling predictor (e.g., converting to 0 mean and 1 standard deviation)
  - transforming predictors (e.g., log)

## Step 3: Specify model and workflow

  - Define model type: linear regression, regression tree, neutral net, etc.
  - Define model engine: particular R package (`lm`, `ranger`, etc.)
  - Define model mode: regression or classification
  - Define workflow: combine recipe and model definition

## Step 4: Train model

  - Tune hyper-parameters: hyper-parameters are configuration settings the govern how a particular ML method is fit to data. They are called "hyper" because "regular" parameters are the parameter within the model that are learned by the ML method.  For example, a method called "random forecast" requires a hyper-parameter that controls the minimum size of a regression tree that is allowed.  This parameter (called `min_n`) could be directly provided by you or could be tuned.  The tuning process involves repeatedly fitting the model using different values of the hyper-parameter and using the hyper-parameter values that yield the best fit to the data. Importantly: not all ML methods have hyper-parameter (e.g., linear regression using the `lm` engine does not have hyper-parameters)
  - Fit model (using best hyper-parameter if they are tuned).  The model is fit to the training data.

## Step 5: Predict

  - Predict testing data (MORE TEXT)
  
## Step 6: Evaluate model

  - Use the appropiate metrics (MORE TEXT)
  
## Step 7: Deploy model

  - Predict new data (MORE TEXT)
  
# Application: Predicting biomass in NEON data

[ADD MOTIVATING TEXT]

```{r message=FALSE}
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
```

## Step 1: Obtain data

The data are from the National Ecological Observatory Network, the DayMet project (meterology data), and NASA's MODIS satellite.  They have been combined to provide the woody vegetation carbon stocks for each NEON plot and the associated meterology, land-use classification, and normalized vegetation index (from MODIS).

```{r}
biomass_data <- read_csv("data/neon_biomass.csv", show_col_types = FALSE)
```

```{r}
biomass_data <- biomass_data |> 
  select(plotID, nlcdClass, plot_kgCm2)
```

## Step 2:  Pre-process data

### Split data into training/testing sets 
Split data
why 0.8
why strata

```{r}
split <- initial_split(biomass_data, prop = 0.80, strata = nlcdClass)
```

```{r}
split
```

```{r}
train_data <- training(split)
test_data <- testing(split)
```

```{r}
train_data
```

### Feature engineering using a recipe

 - requires starting dataset that is used to provide the columns (it isn't used for fitting)
 - a formula with the dependent variable and the predictors.  If `.` is used as the predictors, that means use all columns other than the dependent variable.
 - Steps that modify the data

```{r}
biomass_recipe <- biomass_data |> 
  recipe(plot_kgCm2 ~ . ) |> 
  step_rm(plotID) |>
  step_other(nlcdClass) |>
  step_dummy(nlcdClass)
```

```{r}
biomass_recipe
```

## Step 3: Specify model, engine, and workflow

This creates the model object.  In this example, we are using `linear_reg` with the mode of `regression` (as opposed to `classification`).  Set mode for a linear regression is actually not necessary because it only allows regressions.  The engine is `lm` because we are using the standard R function `lm`.  There are a ton of other functions for linear regression modeling that we could use.  They would be specifed as a different engine.

```{r}
linear_mod <- 
  linear_reg(mode = "regression") |> 
  set_engine("lm")
```

```{r}
linear_mod 
```

We now combine the model and the recipe together to make a workflow that can be used to fit the training and testing data.  `workflow()` initiates the workflow and add_model and add_recipe add those components to the workflow.

```{r}
biomass_wflow <-
  workflow() %>%
  add_model(linear_mod) %>%
  add_recipe(biomass_recipe)
```

You can see that the workflow object has all the components together

```{r}
biomass_wflow
```

## Step 4: Train model on Training Data

We will use the workflow object to train the model.  We need to provide the workflow object and the dataset to the fit function to fit (i.e. train the model)

```{r}
biomass_fit <- biomass_wflow |> 
  fit(data = train_data)
```

You can see that the fit object is the workflow object + the results of the model fitting

```{r}
biomass_fit
```

## Step 5: Predict Test Data

```{r}
predictions <- predict(biomass_fit, test_data)
```

```{r}
predictions
```

```{r}
pred_test <- bind_cols(test_data, predictions)
```

```{r}
pred_test
```

## Step 6: Evaluate model

```{r}
multi_metric <- metric_set(rmse, rsq, mae)
pred_test |> 
multi_metric(truth = plot_kgCm2, estimate = .pred)
```

## Step 7: Deploy model

Create a variable with your last name

```{r}
team_name <- "ADD YOUR LAST NAME"
```

```{r}
submit_data <- read_csv("data/neon_biomass_new.csv", show_col_types = FALSE) 

new_predictions <- predict(biomass_fit, submit_data)

submit_predicted <- bind_cols(submit_data, new_predictions) |>
  mutate(team_name = team_name) |>
  select(plotID, team_name, .pred)

write_csv(submit_predicted, file = "predictions.csv")
```

# Assignment

Your assignment is divided into two R markdown files that use data that you have already seen in the previous modules.

1) `assignment-part-1.Rmd` challenges you to translate the linear regression that you did in the lake ice module follow the tidymodel machine learning format.

2) `assignment-part-2.Rmd` challenges you to develop your own machine learning model to predict the carbon stock data from the forest carbon module.




